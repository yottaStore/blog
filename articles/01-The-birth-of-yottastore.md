# Introduction

Ever felt trapped by your database?  This is the story of how one engineer's struggle with scaling, cost, and 
inflexible schemas led to the creation of YottaStore, a new approach to databases.  This series will document the 
philosophy, architectural choices, and (hopefully!) the growth of YottaStore, as we seek contributors and users.

# How YottaStore was born

It was 2020, and I was a backend engineer at a fast-growing unicorn startup. I *thought* I knew databases – after 10 
years, I hadn't faced a problem I couldn't solve with my trusty toolkit (MongoDB, Postgres, Kafka, and Redis). 
I was, of course, arrogantly wrong.

My company had just experienced massive growth, and then – the pandemic hit. Engineering leadership, facing market 
uncertainty, made cost-cutting a top priority. The biggest target? Our database infrastructure.

We were running a microservices architecture, each service with its own Postgres database. These databases had been 
scaled up *dramatically* to handle traffic spikes, and the costs were becoming unsustainable. Engineering leadership's 
solution: migrate to DynamoDB. New services would use DynamoDB from the start, and existing services would be migrated 
over time.

But the business still demanded new features and the agility that a startup needs. This clashed directly with DynamoDB's 
strict schema requirements. I was tasked with making *every* field in our non-relational data indexed and searchable, 
across massive datasets.

I felt stuck. DynamoDB scans were slow and expensive, and the limits on indexes (even tighter back then) were a major 
roadblock. I believed the high Postgres costs stemmed from technical debt accumulated during our hyper-growth phase. 
Forcing non-relational data into Postgres had led to rushed schema designs and poorly optimized queries, all in an 
attempt to keep business leaders happy.

In hindsight, I'm incredibly grateful that my engineering director resisted my push for MongoDB and forced me out of 
my comfort zone. I'm also glad I stumbled upon the fantastic book, 
[Designing Data-Intensive Applications](https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/), 
which greatly helped me in finding a solution.

# The first iteration: DynamoDB with Postgres

Inspired by the book, and particularly the section on Dynamo, I started exploring how to adapt its principles to our 
challenge.  The core problem was storing and querying *highly heterogeneous* data. To illustrate, imagine we had data 
on various types of vehicles, each with different models, and each combination (type + model) resulting in a unique 
data structure.

We faced a dual challenge: supporting both OLTP and OLAP-style workloads on the *same* dataset.

*   **OLAP (Analytical Processing):**  Operations teams needed to analyze the data to identify which groups of 
vehicles required service and plan the most efficient routes – essentially, large-scale reporting and planning.
*   **OLTP (Transactional Processing):**  Once a vehicle was serviced, technicians needed to update its 
record with maintenance details – frequent, small updates.

Adding to the complexity, our data quality was inconsistent. For example, dates were stored in multiple formats, 
and similar inconsistencies plagued other fields. This is a common challenge in rapidly growing startups, where speed 
often trumps perfect data hygiene.

Then came the first "eureka" moment: tackling the indexing problem.  Instead of relying solely on DynamoDB's 
expensive indexes, I could use Postgres to store a *normalized and sanitized* subset of the data – essentially, metadata.

These Postgres tables, organized by geographic area, would be updated *asynchronously* from a stream of events 
generated by DynamoDB (our source of truth).  This meant the index might be slightly behind the primary data, but it 
provided *fast, cost-effective* indexed searches – a huge improvement over DynamoDB's limitations.

The second "eureka" moment involved efficient data scans. DynamoDB's full table scans were slow and expensive. Inspired 
by [Harris's](https://www.cl.cam.ac.uk/research/srg/netos/papers/2001-caslists.pdf)
work on wait-free data structures, I realized I could leverage DynamoDB's compare-and-swap (CAS) operations 
to build linked lists *within* DynamoDB itself.

These linked lists would allow me to scan only the *relevant subset* of data, avoiding full table scans. This was a 
complex approach, but it opened up exciting possibilities for more efficient data access within the DynamoDB environment.  
This is where I really started diving deep into the potential of the [Dynamo paper](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf).

It's worth noting that this entire system was built in Node.js. While I successfully applied this design pattern 
to other problems at different companies, I became increasingly aware of Node.js's limitations, particularly for 
in-memory operations.  Furthermore, the fact that AWS DynamoDB wasn't a true implementation of the original Dynamo 
paper – which I had come to deeply appreciate – fueled my desire for something better.

# The second iteration: Dynamo and Golang

To address Node.js's limitations, I switched to Golang for its performance and concurrency features.  My goal was 
still to build a system closer to the original Dynamo paper's principles. For this iteration, I experimented with 
using Amazon S3 as the underlying storage layer.  This provided greater flexibility, but S3's cost, particularly for 
operations like storing partial indexes to accelerate searches, became a concern.

~
To reduce S3 costs, I explored using locally attached NVMe drives as a cache. This, however, plunged me into the 
complexities of direct storage management. To interact directly with the NVMe hardware and bypass the operating 
system's file system overhead, I had to use low-level Linux system calls like `ftruncate` (for resizing files), 
`writev` (for writing multiple data buffers at once), and the `O_DIRECT` flag 
(for bypassing the kernel's page cache).  Within this cache, I implemented partial indexes 
using Log-Structured Merge Trees (LSM trees), a data structure well-suited for handling frequent writes.

I took a look at using NVMe attached storage as a cache, to save costs compared to always hitting S3, and this opened
me the world of linux syscalls like `ftruncate`, `writev`, `O_DIRECT` flags and many other nightmarish tools I needed
to use if I wanted to store efficiently the cached data. At this point I was building partial indexes in the cache, using
LSM trees to efficiently handle updates.

~
This led to a radical rethinking of the architecture: What if we eliminated S3 entirely and relied solely on NVMe 
drives?  The cost and complexity of managing the S3/NVMe interaction were becoming significant. My idea was to treat 
the NVMe drives as a form of persistent, byte-addressable memory, similar to RAM but with different performance 
characteristics.  After all, an NVMe drive is essentially a large, 0-indexed array of 4KB blocks, allowing 
for random access.  This model would allow me to leverage the extensive research on in-memory, wait-free algorithms, 
but applied to persistent storage. In this model I could reuse the large academic literature about in memory wait
free algorithms, imagining a collection...

This is where I came up with the idea of making another large architectural jump: What if we dropped S3, and instead
used NVMe without a filesystem, treating it as the RAM in a very large distributed machine. After all NVMe disks are 
0-indexed arrays of 4 kb cells of memory, which can be randomly accessed, exactly like RAM albeit with slower performance.
In this model I could reuse the large academic literature about in memory wait free algorithms, imagining a collection...

~
To realize this vision, I needed a high-performance interface for managing concurrent operations on both the NVMe 
drives and the network. The [NVMe ZNS specification](https://nvmexpress.org/specifications/) offered a promising 
approach, providing a rich API to maximize NVMe performance. For asynchronous I/O operations, I turned to 
the [io_uring](https://github.com/axboe/liburing) Linux kernel interface.

What I needed was a highly performant interface to handle highly concurrent operations on NVMe and network. In my help
came the [NVMe ZNS specification](https://nvmexpress.org/specifications/), which provides a rich API to use NVMe storage
to the best of its performance, and the [io_uring](https://github.com/axboe/liburing) linux kernel interface

~
However, integrating `io_uring`, the NVMe ZNS API, and Golang proved incredibly challenging.  Golang's abstractions, 
while generally beneficial, made it difficult to work with the low-level, memory-aligned operations required by 
`io_uring` and NVMe.  This forced me to write significant amounts of unidiomatic Go code, sacrificing some of the 
language's elegance and safety.

The problem was that making `io_uring`, the NVMe API and golang work together nicely proved to be extremely
difficult. I eventually made it several months later, but Golang forced me to write a lot of unidiomatic code, 
for example to handle memory aligned operations.

And so, once again, I found myself at an impasse...