# Introduction

Ever felt trapped by your database?  This is the story of how one engineer's struggle with scaling, cost, and 
inflexible schemas led to the creation of a new approach to databases. This series will document the 
philosophy, architectural choices, and (hopefully!) the growth of [Yottastore](https://github.com/yottaStore/documentation), 
as we seek contributors and users.

# How Yottastore was born

It was 2020, and I was a backend engineer at a fast-growing unicorn startup. I *thought* I knew databases – after 10 
years, I hadn't faced a problem I couldn't solve with my trusty toolkit (MongoDB, Postgres, Kafka, and Redis). 
I was, of course, arrogantly wrong.

My company had just experienced massive growth, and then – the pandemic hit. The c-suites, facing market 
uncertainty, made cost-cutting a top priority. The biggest target? Our database infrastructure.

We were running a microservices architecture, each service with its own Postgres database. These databases had been 
scaled up *dramatically* to handle traffic spikes, and the costs were becoming unsustainable. Engineering leadership's 
solution: migrate to DynamoDB. New services would use DynamoDB from the start, and existing services would be migrated 
over time.

But the business still demanded new features and the agility that a startup needs. This clashed directly with DynamoDB's 
strict schema requirements. I was tasked with making *every* field in our non-relational data indexed and searchable, 
across massive datasets.

I felt stuck. DynamoDB scans are slow and expensive, and the limits on indexes (even tighter back then) were a major 
roadblock. I believed the high Postgres costs stemmed from technical debt accumulated during our hyper-growth phase: 
forcing non-relational data into Postgres had led to rushed schema designs and poorly optimized queries, all in an 
attempt to keep business leaders happy.

In hindsight, I'm incredibly grateful that my engineering director resisted my push for MongoDB and forced me out of 
my comfort zone. I'm also glad I stumbled upon the fantastic book, 
[Designing Data-Intensive Applications](https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/), 
which greatly helped me in finding a solution, and is the source of inspiration for many ideas you'll find in this series.

# The first iteration: DynamoDB with Postgres

Motivated by the book, and particularly the section on Dynamo, I started exploring how to adapt its principles to our 
challenge.  The core problem was storing and querying *highly heterogeneous* data. To illustrate, imagine we had data 
on various types of vehicles, each with different models, each with different revisions, and each combination 
(type, model and revision) resulting in a unique data structure, this over the span of hundreds of thousands vehicles.

We faced a dual challenge: supporting both OLTP and OLAP-style workloads on the *same* dataset.

*   **OLAP (Analytical Processing):**  Operations teams needed to analyze the data to identify which groups of 
vehicles required service and plan the most efficient routes – essentially, large-scale reporting and planning.
*   **OLTP (Transactional Processing):**  Once a vehicle was serviced, technicians needed to update its 
record with maintenance details – frequent, small updates.

Adding to the complexity, our data quality was inconsistent. For example, dates were stored in multiple formats or under
different columns, and similar inconsistencies plagued other fields. This is a common challenge in rapidly growing 
startups, where speed often trumps perfect data hygiene.

# Efficient scans

Then came the first "eureka" moment: tackling the indexing problem. Instead of relying solely on DynamoDB's 
expensive indexes, I could use Postgres to store a *normalized and sanitized* subset of the data – essentially, metadata.

These Postgres tables, organized by geographic area, would be updated *asynchronously* from a stream of events 
generated by DynamoDB (our source of truth). This meant the index might be slightly behind the primary data, but it 
provided *fast, cost-effective* indexed searches – a huge improvement over DynamoDB's limitations.

// TODO: Talk more about indexes and scans
// TODO: make this more digestible
The second "eureka" moment involved efficient data scans. DynamoDB's full table scans were slow and expensive. Inspired 
by [Harris's](https://www.cl.cam.ac.uk/research/srg/netos/papers/2001-caslists.pdf)
work on wait-free data structures, I realized I could leverage DynamoDB's compare-and-swap (CAS) operations 
to build linked lists *within* DynamoDB itself.

These linked lists would allow me to scan only the *relevant subset* of data, avoiding full table scans. This was a 
complex approach, but it opened up exciting possibilities for more efficient data access within the DynamoDB environment.  
This is where I really started diving deep into the potential of the [Dynamo paper](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf).

It's worth noting that this entire system was built in Node.js. While I successfully applied this design pattern 
to other problems at different companies, I became increasingly aware of Node.js's limitations, particularly for 
in-memory operations.  Furthermore, the fact that AWS DynamoDB wasn't a true implementation of the original Dynamo 
paper – which I had come to deeply appreciate – fueled my desire for something better.

